* list of tensorflow features not used
- initializer
- regularizer
- collections
- custom_getter
- variable search
- optimizers
* how to do inits?
- either:
  - change initializer_op
    - cons
      - relies on implementation details
      - might break with weird partition stuff
  - use initializer
    - cons
      - ugly interface (w/ metadata)
        - variable isn't created yet
  - different step to initialize
    - cons
      - harder to do conditional initialization
        - eg. orth init only for RNN matrix
      - additional step
- solution
  - use initializer
  - pass in metadata as dict instead
* extended graph
- either:
  - make graph wrapper
  - use existing graph and add attributes
- to store graph + extra state
  - eg.
    - metadata
    - state for specific tricks
      - eg. step counter for gradnets
- solution:
  - make graph wrapper
* custom get_variable
- set default to always 0s
  - instead of tensorflow's default magic one
  - rationale: make it easier to spot initialization bugs
- make it hookable
  - previous use cases for hooking get_shared / get_variable:
    - meta adam
    - weight normalization
    - inits
  - arguably, get_variable is the wrong place to hook these things, but it is convenient, so we leave it hookable
* less variable_scope/get_variable kwargs
- some of the tensorflow defaults are hacky and not the best to use
  - eg.
    - collections
    - regularizer
- some require hacks to work with metdata
  - eg.
    - trainable affecting collection, while it would be better to search via metadata
- solution: make these unavailable and use metadata search instead
  - implement the things we need as we need them
  - additional benefit: don't have to support what seems like a constantly changing interface
* initializer functions vs initial_values
- pros of making all initializers return tensors/numpy arrays
  - easier composability
    - eg. easier to make a scale by 2 hook
  - cleaner init code
- seems like we get nothing from having initializer functions
  - the original rationale seems to be to have a function from metadata to value
- also allows numpy initializers (eg. orthogonal) to behave the same way as more basic ones
  - eg. heterogeneous inits could cause issues with random seed-ing
* update format
- doing it as op
  - pros
    - simplest to use
  - cons
    - it's opaque and can't be transformed/inspected
- accumulating pairs
  - pros
    - most general
  - cons
    - ugliest code
      - eg. need to accumulate in a for loop
- having an accumulator that maintains an OrderedDict
  - pros
    - update code is almost as clean as doing it as an op
  - cons
    - accumulator code is specific to updates
      - ideally we want to use the same-ish for monitoring
- solution:
  - accumulator that maintains an OrderedDict + shared accumulator base class
