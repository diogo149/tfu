* how to do inits?
- either:
  - change initializer_op
    - cons
      - relies on implementation details
      - might break with weird partition stuff
  - use initializer
    - cons
      - ugly interface (w/ metadata)
        - variable isn't created yet
  - different step to initialize
    - cons
      - harder to do conditional initialization
        - eg. orth init only for RNN matrix
      - additional step
- solution
  - use initializer
  - pass in metadata as dict instead
* extended graph
- either:
  - make graph wrapper
  - use existing graph and add attributes
- to store graph + extra state
  - eg.
    - metadata
    - state for specific tricks
      - eg. step counter for gradnets
- solution:
  - make graph wrapper
* custom get_variable
- set default to always 0s
  - instead of tensorflow's default magic one
  - rationale: make it easier to spot initialization bugs
- make it hookable
  - previous use cases for hooking get_shared / get_variable:
    - meta adam
    - weight normalization
    - inits
  - arguably, get_variable is the wrong place to hook these things, but it is convenient, so we leave it hookable
* less variable_scope/get_variable kwargs
- some of the tensorflow defaults are hacky and not the best to use
  - eg.
    - collections
    - regularizer
- some require hacks to work with metdata
  - eg.
    - trainable affecting collection, while it would be better to search via metadata
- solution: make these unavailable and use metadata search instead
  - implement the things we need as we need them
  - additional benefit: don't have to support what seems like a constantly changing interface
